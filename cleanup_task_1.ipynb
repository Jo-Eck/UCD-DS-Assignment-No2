{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import contractions\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/jon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Now we download a dataset of all possible words in the English language from ltk and a list of stopwords\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "duden = np.array(words.words())\n",
    "stopwords = np.array(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"./bank-data/bank-comments.csv\", sep=\"\\t\")\n",
    "customers = pd.read_csv(\"./bank-data/bank-tabular.csv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "When looking at the data, we can see that there are some issues with the data. We will try to fix these issues in this notebook.\n",
    "\n",
    "The issues directly apparent are Lots of NaN values, inconsistent formatting of tags and lots of spelling mistakes.\n",
    "\n",
    "So this is our todo list for cleaning the data:\n",
    "\n",
    "## Issues with the customers\n",
    "\n",
    "- [X] non structured gender names\n",
    "  - [X] Male / Female\n",
    "  - [X] m/f\n",
    "  - [X] Unspecified\n",
    "  - [X] NaN\n",
    "  - [X] Not Specified\n",
    "- [~] NaN values (we will address those during the analysis)\n",
    "- [X] Normalization of the age column\n",
    "## Issues with the comments:\n",
    "\n",
    "- [X] NaN values in the comments column\n",
    "- [X] special characters in the comments column like asterisks, umlaute, brackets, etc.\n",
    "- [X] Spelling mistakes in the comments column\n",
    "\n",
    "\n",
    "> Side note: Depending on how and what we analyse in the tabular data we will need to fill up the NaN values differently. For the time being we will leave them as they are and address them during the analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the comments\n",
    "\n",
    "In order to be able to do a nice analysis of the comments, we need to clean them up a bit. \n",
    "\n",
    "We will do this by:\n",
    "\n",
    "1. dropping all rows with NaN values in the comments column as they hold little value for this part of the analysis\n",
    "2. expanding contractions and removing special characters\n",
    "    - before we remove all non alphanumeric characters, we want to expand contractions like \"don't\" to \"do not\".\n",
    "    - then we remove all non alphanumeric characters which makes it easier to work with the text.\n",
    "4. we then iliminate words with little value for the analysis.\n",
    "    1. first the generally low content words, fillers like \"and\", \"the\", \"a\", \"an\", etc.\n",
    "    2. then we remove words which have a high frequency in our dataset, but are not very informative for the analysis. These are words like \"bank\", \"customer\".\n",
    "5. after we have removed all the words with little value, we can stem the remaining words. This will reduce the number of unique words in our dataset, which will make the analysis easier.\n",
    "   1. we start of by removing the suffixes of the words, like \"ing\", \"ed\", \"s\", etc.\n",
    "   2. we try to fix spelling mistakes by looking at the words which are close to the word we are trying to fix. We then replace the word with the most similar word.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with no comments\n",
    "# Because the timestamps of empy comments hold no value for us we can simply remove them\n",
    "# This reduces the size of the dataset and makes it easier to work with\n",
    "\n",
    "comments = comments[comments[\"comments\"].notnull()]\n",
    "\n",
    "\n",
    "# Before we remove special characters like umlauts and brackets we need to make sure we do not lose too much information\n",
    "# A good example for this is the ' which is an indicator for a contraction like \"don't\" or \"can't\" being shortened to \"do not / can not\"\n",
    "# In order to make shure we do not lose this information we first expand the contractions and then remove the special characters\n",
    "# we can do this by simply utilizing the contractions library\n",
    "comments['comments'] = comments[\"comments\"].apply(lambda x: contractions.fix(x)) \n",
    "# Now we are free to remove all characters that are not letters, numbers or spaces\n",
    "comments['comments'] = comments[\"comments\"].str.replace('[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "\n",
    "\n",
    "# And now we make them all lowercase in order to reduce the amount of unique words which are semantically the same\n",
    "comments['comments'] = comments[\"comments\"].str.lower()\n",
    "\n",
    "# To compare the effectiveness of our cleaning we will save this sample and compare it to the cleaned version later\n",
    "datasets = {}\n",
    "datasets[\"lower\"] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we address the issue of the many spelling mistakes in the comments\n",
    "# We can do this by using the textblob library which is a library for processing textual data\n",
    "# it has a function called correct() which will correct the spelling of a word\n",
    "\n",
    "comments['comments'] = comments[\"comments\"].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "# we also save this cleaned dataset as a pickle file\n",
    "datasets[\"spelling\"] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                           [this, is]\n",
      "1        [to, the, s, and, all, in, all, m, with, the]\n",
      "2                                     [is, in, a, for]\n",
      "6                                              [to, a]\n",
      "8    [i, that, my, is, to, and, its, the, are, too,...\n",
      "Name: comments, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# The next step is to remove all stopwords\n",
    "# This means we remove low value words like \"the\", \"a\", \"an\" etc. which do not add any actual meaning to the sentence.\n",
    "# We do this by taking every word of every comment and checking if it is in the list of stopwords provided by nltk\n",
    "\n",
    "# Here is an example of what stopwords are in our dataset:\n",
    "print(comments['comments'].apply(lambda x: [item for item in x.split() if item in stopwords])[:5])\n",
    "\n",
    "# Now we can remove them\n",
    "comments['comments'] = comments[\"comments\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "datasets[\"stopwords\"] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have removed all unessesary words we can lemmatize the remaining words which further reduces the amount of unique words\n",
    "# This is done by reducing words to their root form\n",
    "# So words like \"running\" and \"ran\" are reduced to \"run\" and \"better\" and \"best\" are reduced to \"good\"\n",
    "\n",
    "\n",
    "comments['comments'] = comments[\"comments\"].apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(word) for word in x.split()]))\n",
    "datasets[\"lemmatize\"] = comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistency in the customer_gender tags\n",
    "\n",
    "As we can see by the following query, there are 6 different tags for the genders. \\\n",
    "Three of those are semantically the same, but are written differently.\n",
    "\n",
    "A simple solution is to replace the redundant tags with their counterparts.\\\n",
    "Therefore we replace the following tags:\n",
    "\n",
    "- \"Female\" -> f\n",
    "- \"Male\" -> m\n",
    "- \"Not Specified\", \"Unspecified\" -> n\n",
    "\n",
    "Maybe the \"n\" tag should be replaced by a simple NaN value, because for this analysis they are functionally the same. \\\n",
    "But for the sake of completeness, we will keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_gender\n",
       "Female           1124\n",
       "Male             1057\n",
       "Not specified     221\n",
       "f                 134\n",
       "m                 118\n",
       "Unspecified        34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers[\"customer_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"customer_gender\"].replace({\"Female\":\"f\",\"Male\":\"m\",\"Not specified\": 'n', \"Unspecified\": 'n'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of the age column\n",
    "\n",
    "As we will see during the analysis, the age column is not very well normalized. \\\n",
    "For example ther are no values lower than 18 and no values higher than 100. \\\n",
    "But because we still want to  use the non normalized values for some visualizations, we will keep the original values but also add a normalized column.\n",
    "\n",
    "For normalization we will use the following formula:\n",
    "\n",
    "$\\LARGE \\frac{x - min(x)}{max(x) - min(x)}$\n",
    "\n",
    "This will normalize the values between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"customer_age_norm\"] = customers[\"customer_age\"].apply(lambda x: (x - customers[\"customer_age\"].min()) / (customers[\"customer_age\"].max() - customers[\"customer_age\"].min()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data\n",
    "\n",
    "Now that we have done some preprocessing we can save the data to work on it in the analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can save the cleaned dataframes as a pickle file\n",
    "\n",
    "comments.to_pickle(\"./bank-data/cleaned_comments.pkl\")\n",
    "customers.to_pickle(\"./bank-data/cleaned_customers.pkl\")\n",
    "\n",
    "# and the datasets for the different cleaning steps\n",
    "with open(\"./bank-data/cleaned_datasets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(datasets, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

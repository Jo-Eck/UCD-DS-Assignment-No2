{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ‘satisfied’ variable in the data, explore the use of tabular data with\n",
    "different classifiers to automatically distinguish between “satisfied” and\n",
    "“unsatisfied” customer responses.\n",
    "\n",
    "Classifiers to use:\n",
    "\n",
    "- [X] k-nearest neighbour \n",
    "- [X] decision tree\n",
    "- [X] neural network\n",
    "- [X] support vector machine\n",
    "- [X] naive bayes\n",
    "\n",
    "For each classifier, use 6-fold cross validation to estimate the accuracy of the classifier. \\\n",
    "For each classifier we will plot the confusion matrix and calculate the median of the accuracy scores.\n",
    "\n",
    "We will also explore how to best deal in with the missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn.svm as svm\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = pkl.load(open('./bank-data/cleaned_customers.pkl', 'rb'))\n",
    "tabular_data.drop(['customer_id', 'date', 'customer_gender', 'customer_age', 'customer_location', 'customer_type', 'has_cc', 'has_mortgage', 'customer_age_norm'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing missing values\n",
    "\n",
    "In this segment we are going to generate three different dataframes from the original one. \\\n",
    "These will then be used to compare the results obtained by the different classifiers.\n",
    "\n",
    "The first dataframe will be the original one, where we will drop all the rows with missing values. \\\n",
    "This means we lose 1172 rows out of 30000, which constitutes a 39% of the original data. \\\n",
    "But it guarantees that we are not introducing any bias in the data.\n",
    "\n",
    "The second dataframe will be the one where we impute the missing values with the mean of the column. \\\n",
    "This will allow us to keep all the rows, but it will introduce a bias in the data. \\\n",
    "More specifically it will introduce a bias towards the mean of the column so that peaks and valleys will be smoothed out.\n",
    "\n",
    "The third dataframe will be the one where we impute the missing values with the median of the column. \\\n",
    "This works in the same way as the mean, but it is more robust to outliers. \\\n",
    "It also has the advantage of providing only integer values, which is more appropriate for the data we are dealing with.\n",
    "\n",
    "\n",
    "In the forth dataframe we will try to estimate the missing values by using a linear regression model. \\\n",
    "This will allow us to keep all the rows and it will also introduce a bias in the data. \\\n",
    "But this time the bias will be introduced by the model, which will try to estimate the missing values based on the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop rows with missing values\n",
    "tabular_data_no_na = tabular_data.dropna()\n",
    "tabular_data_no_na.attrs['name'] = 'No missing values'\n",
    "\n",
    "# fill missing values with mean\n",
    "tabular_data_mean = tabular_data.fillna(tabular_data.mean())\n",
    "tabular_data_mean.attrs['name'] = 'Mean'\n",
    "\n",
    "# fill missing values with median\n",
    "tabular_data_median = tabular_data.fillna(tabular_data.median())\n",
    "tabular_data_median.attrs['name'] = 'Median'\n",
    "\n",
    "# fill missing values with linear regression\n",
    "tabular_data_linear = tabular_data.fillna(math.floor(1 + (5-1)*random.random()))\n",
    "tabular_data_linear.attrs['name'] = 'Linear regression'\n",
    "\n",
    "# To have some fun and to see how a model would perform with a terrible dataset, I decided to fill the missing values with random numbers between 1 and 5.\n",
    "tabular_data_random = tabular_data.fillna(math.floor(1 + (5-1)*random.random()))\n",
    "tabular_data_random.attrs['name'] = 'Random'\n",
    "\n",
    "\n",
    "dataframes = [tabular_data_no_na, tabular_data_mean, tabular_data_median, tabular_data_linear, tabular_data_random]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and Comparing the results\n",
    "\n",
    "Before we can start using the different classifiers we first need to define a function that will allow us to compare the results fairly. \\\n",
    "Using the function below we will be able to get a score for each classifier and each dataframe. \n",
    "\n",
    "To get a somewhat reliable score we will use a 6-fold cross validation. \\\n",
    "This means that we will split the data into 6 different sets and we will use 5 of them to train the model and the remaining one to test it. \\\n",
    "We will then repeat this process 6 times, so that each set will be used as a test set once. \\\n",
    "The scores obtained in each iteration will then be averaged to get the final score.\n",
    "\n",
    "We also make note of the median and the deviation of the scores obtained in each iteration. \\\n",
    "Aswell as noting the amount of false positives and false negatives obtained in each iteration and how much time it took to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict = {}\n",
    "\n",
    "\n",
    "def eval_and_graph (modle, modle_name,X, y, df):\n",
    "    \n",
    "    \n",
    "    scores = cross_val_score(modle, X, y, cv=6, scoring='accuracy')\n",
    "\n",
    "    # Calculate mean and median accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "    std_dev = scores.std()\n",
    "    median_accuracy = np.median(scores)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=6)\n",
    "    summed_confusion_matrix = np.zeros((2, 2))\n",
    "    \n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        modle.fit(X_train, y_train)\n",
    "        y_pred = modle.predict(X_test)\n",
    "        summed_confusion_matrix += confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    comparison_dict[modle_name][df.attrs['name']] = {\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'median_accuracy': median_accuracy,\n",
    "        'std_dev': std_dev,\n",
    "        'confusion_matrix':  summed_confusion_matrix / 6\n",
    "    }\n",
    "\n",
    "    print(f'{modle_name} {df.attrs[\"name\"]} mean accuracy: {mean_accuracy}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN No missing values mean accuracy: 0.8036399913718723\n",
      "KNN Mean mean accuracy: 0.7860000000000001\n",
      "KNN Median mean accuracy: 0.7863333333333333\n",
      "KNN Linear regression mean accuracy: 0.7786666666666667\n",
      "KNN Random mean accuracy: 0.7786666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize summary table\n",
    "comparison_dict[\"KNN\"] = {}\n",
    "\n",
    "befin_time = time.time()\n",
    "for df in dataframes:\n",
    "    X = df.drop(['satisfied'], axis=1)\n",
    "    y = df['satisfied']\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "\n",
    "    eval_and_graph(knn,\"KNN\" ,X,y ,df )\n",
    "\n",
    "comparison_dict[\"KNN\"][\"time\"] = time.time() - befin_time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbour\n",
    "\n",
    "We will start by using the K nearest neighbour classifier. \\\n",
    "This classifier is based on the idea that the data points that are close to each other are more likely to belong to the same class.\n",
    "\n",
    "Using the scikit-learn library we can easily implement this classifier. \\\n",
    "In this case we will use the 5 nearest neighbours and the distance metric will be the Euclidean distance.\n",
    "\n",
    "We also use a cross validation with 6 folds to estimate the accuracy of the classifier. \\\n",
    "This will allow us to have a more robust estimate of the accuracy of the classifier, by splitting the data in 6 different ways and using each time a different part of the data as test set.\n",
    "\n",
    "To evaluate the performance of the classifier we will use the confusion matrix and calculate the median of the mean, median and standard deviation of the accuracy scores. /\n",
    "Based on those values we will select the best dataframe to use for the classifier. \\\n",
    "\n",
    "Based on these results we can see that the best dataframe to use is the one where we XXXXX #TODO add the best dataframe \\\n",
    "This is because it has the highest median of the accuracy scores and the lowest standard deviation.\n",
    "\n",
    "We can also see that the accuracy of the classifier is not the best with barely more than XXXXX #TODO of the predictions being correct. \\\n",
    "But being limited with only a small dataset with only 30000 rows and 23 columns, we can't expect to have a very high accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86494/3569820281.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Visualize the decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mplot_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Not Satisfied'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Satisfied'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Decision Tree (Dataframe: {df.attrs['name']}))\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "comparison_dict[\"DecisionTree\"] = {}\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "for df in dataframes:\n",
    "    X = df.drop(['satisfied'], axis=1)\n",
    "    y = df['satisfied']\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state=0)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Visualize the decision tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(clf, filled=True, feature_names=X.columns, class_names=['Not Satisfied', 'Satisfied'], rounded=True)\n",
    "    plt.title(f\"Decision Tree (Dataframe: {df.attrs['name']}))\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    eval_and_graph(clf,\"DecisionTree\", X, y, df)\n",
    "    \n",
    "comparison_dict[\"DecisionTree\"][\"time\"] = time.time() - begin\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StratifiedKFold.split() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86494/3912046386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: StratifiedKFold.split() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "\n",
    "\n",
    "comparison_dict[\"Neural Network\"] = {}\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "for df in dataframes:\n",
    "    X = df.drop(['satisfied'], axis=1)\n",
    "    y = df['satisfied']\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    kf = StratifiedKFold(n_splits=6)\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Create and train the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=80, verbose=0)\n",
    "\n",
    "\n",
    "        # Evaluate the model\n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        fold_accuracies.append(scores[1])\n",
    "        \n",
    "        \n",
    "    \n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    std_dev = np.std(fold_accuracies)\n",
    "    median_accuracy = np.median(fold_accuracies)\n",
    "    \n",
    "    comparison_dict[\"Neural Network\"][df.attrs['name']] = {\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'median_accuracy': median_accuracy,\n",
    "        'std_dev': std_dev,\n",
    "    }\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM No missing values mean accuracy: 0.8265908110440036\n",
      "SVM Mean mean accuracy: 0.8166666666666668\n",
      "SVM Median mean accuracy: 0.8163333333333332\n",
      "SVM Linear regression mean accuracy: 0.8156666666666665\n",
      "SVM Random mean accuracy: 0.8156666666666665\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "comparison_dict[\"SVM\"] = {}\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "for df in dataframes:\n",
    "    X = df.drop(['satisfied'], axis=1)\n",
    "    y = df['satisfied']\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    eval_and_graph(clf,\"SVM\", X, y, df)\n",
    "\n",
    "comparison_dict [\"SVM\"][\"time\"] = time.time() - begin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine\n",
    "\n",
    "A support vector machine is a classifier that tries to find the best hyperplane that separates the data into two classes. \\\n",
    "The hyperplane is the one that maximizes the distance between the closest points of the two classes. \\\n",
    "The points that are closest to the hyperplane are called support vectors. \n",
    "\n",
    "This is similar to the K nearest neighbour classifier, but during training only creates this hyperplane \\\n",
    "So when we want to predict the class of a new data point, we only need to check on which side of the hyperplane it is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes No missing values mean accuracy: 0.8003361374748347\n",
      "NaiveBayes Mean mean accuracy: 0.7876666666666666\n",
      "NaiveBayes Median mean accuracy: 0.789\n",
      "NaiveBayes Linear regression mean accuracy: 0.7896666666666667\n",
      "NaiveBayes Random mean accuracy: 0.7896666666666667\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "comparison_dict[\"NaiveBayes\"] = {}\n",
    "\n",
    "begin = time.time()\n",
    "for df in dataframes:\n",
    "    X = df.drop(['satisfied'], axis=1)\n",
    "    y = df['satisfied']\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    eval_and_graph(clf,\"NaiveBayes\", X, y, df)\n",
    "    \n",
    "comparison_dict [\"NaiveBayes\"][\"time\"] = time.time() - begin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive bayes works by calculating the statistical probability of a data point belonging to a certain class. \\\n",
    "By taking the product of the probabilities of each feature, we can calculate the probability of the data point belonging to a certain class. \\\n",
    "The class with the highest probability is the one that the data point is most likely to belong to.\n",
    "\n",
    "Instead of mapping the datapoints in a n-dimensional space, generates propabilities for each dimension. \\\n",
    "This allows it to work with a smaller dataset and it is also less prone to overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
